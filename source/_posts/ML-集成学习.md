---
title: ML-集成学习
date: 2022年3月12日
categories: [机器学习]
tags: [集成学习，随机森林, Boosting, Bagging]
katex: true
cover: https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/03/20/7f0f94f3ea0db706c233c4bcb413c046-XfMeXNI42d8-b472ff.jpg
top_img: https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/03/20/9d2244833e878e2169062087c9ab0874-wallhaven-g72p87-af7e51.jpg
---

## 集成学习

#### 1. 集体与个体

​		**集成学习：**通过构建并结合多个学习器来完成学习任务。一般结构是先产生一组“个人学习器”，再用某种策略将它们集合起来。一般根据**基学习器**是否同类，分为同质和异质。

​		**一般理论上使用弱学习器作为基学习器足以获得好的性能**(弱学习器常指泛化性能略优于随机猜测的学习器，例如在二分类问题上精度略高于50%的分类器。)，但是实际上还是会使用比较强的学习器。在选择学习器的时候，应该满足**好而不同**，即既要有一定的准确性，又要有多样性。

​		集成学习可以分为两类：

​		1）个体学习器之间存在强依赖关系，必须串行生成的序列化方法，如**Boosting**；

​		2）个体学习器不存在强依赖关系，可同时生成并行化方法，如**Bagging、“随机森林”(Random Forest)**。



#### 2. Boosting

​		Boosting的工作机制是先从初始训练集中训练一个基学习器，然后基于基学习器的表现对训练样本的分布进行调整，使得先前基学习器做错的样本得到更多的关注，然后基于调整后的样本分布训练下一个基学习器。如此重复，直至基学习器数量达到预定值$T$，最终将这$T$个基学习器进行加权结合。

​		著名的Boosting算法是AdaBoost算法。



#### 3. Bagging与随机森林

​		上面可知，好的学习器是好而不同的，应该尽可能相互独立，可以考虑采样多个子集，然后训练不同的基学习器。但是因为每个学习器只用到了一小部分训练数据，甚至不足以有效学习，这样基学习器的效果也不好，所以考虑使用**有叠加的采样子集**。

###### 3.1 Bagging

​		Bagging是并行集成学习的典型，采用自助采样法。

​		1） 给定m个样本，随机选出一个样本放入采样集；

​		2） 把和这个样本放回，使下次采样还有可能被采集。

​		3）重复上面步骤，m次采样后我们得到了含有m个样本的采样集。  

​		利用上面的方法，采样出$T$个含有 $m$ 个训练样本的采样集，然后分别训练一个基学习器，再将这些基学习器进行结合，这就是Bagging的基本流程。与标准的 AdaBoost 只适用二分类任务不同，Bagging能够不经修改的用于多分类。

###### 3.2 随机森林

​		随机森林是Bagging的拓展变体，$RF$ 以决策树作为基学习器构建Bagging集成，同时在决策侧的训练过程中引入了随机属性选择。

具体的，传统的决策树在选择划分属性时，是在当前节点的属性集合（假设有d个）中选择一个最优属性；而 $RF$ 中，对于基决策树的每个节点，先从该节点的属性集合中随机选取包含 $k$ 个属性的子集，然后在从这个子集汇总选择一个最优属性用于划分。如果 $k=d$ ，则基决策树的构建和传统决策树相同；如果 $k=1$ ，则是随机选择一个属性用于划分，一般情况推荐值 $k = log_2d$ 。

#### 4. 结合策略

​		学习器的结合的好处有可能来自以下三个方面：

  		1) 从统计方面来看，学习任务的假设空间一般很大，可能有多个假设在训练集上达到同等性能，如果只是用一个学习器有可能因为误选导致泛化性能不佳；
  		2) 从计算方面来看，学习算法有可能会陷入局部极小，有的局部极小点对应的泛化能力可能很糟糕，通过多次运算进行结合，可以减低陷入糟糕局部极小点的风险。
  		3) 从表示方面来看，某些学习任务的的真实假设可能不在当前学习算法所考虑的假设空间中，使用单学习器肯定无用，使用多个学习器可以扩大假设空间，学到更好的近似。

######## 4.1 平均法

​		对于数值型输出 $ h_i(x)$ ，使用平均法。

  -    简答的平均法
       $$
       H(x) = \frac{1}{T} \sum_{i=1}^T h_i(x)
       $$

-   加权平均法

$$
H(x) = \frac{1}{T} \sum_{i=1}^T w_ih_i(x)
$$

​		其中，$w_i$ 是个体学习器 $h_i$ 的权重，大于$0$，相加为$1$。一般来说，**在个体学习器性能相差较大的时候，宜使用加权平均法，而在个体学习器性能相近时使用简单平均法。**

######## 4.2 投票法

​		对于分类任务而言，常见的结合策略是使用投票法。学习器将从类别标记集合 $\{c_1,c_2,...,c_N\}$ 中预测一个标记。将 $h_i$ 在样本 $x$ 上预测输出表示为一个 $N$ 维向量 $(h_i^1(x);h_i^2(x);...;h_i^N(x))$ ，其中 $h_i^j (x) $ 代表学习器 $h_i$ 在类别标记 $c_j$ 上的输出。

-   **绝对多数投票**
    $$
    H(x) = 
    \begin{cases}
    c_j,&   if\ \sum_{i=1}^Th_i^j(x)>0.5\sum_{k=1}^N\sum_{i=1}^T h_i^k(x); \\
    \\
    reject& \text{otherwise}
    \end{cases}
    $$
    ​		即若某标记的票过半数，则预测为该标记，否则拒绝。这里的半数，其实就是把所有 $h_i^k(x)$ 相加的一般，也就是所有分类器对每个类别标签的预测的总和。提供“拒绝预测”这个选择，对于可靠性要求较高的任务时一个好的机制。

-   **相对多数投票法**
    $$
    H(x)=c_{\mathop{arg\ max}\limits_{j} \sum_{i=1}^Th_i^j(x)}
    $$
    即预测为得票最多的标记，如果有多个最高值，则随机选取一个。

-   **加权投票法**
    $$
    H(x)=c_{\mathop{arg\ max}\limits_{j} \sum_{i=1}^T w_ih_i^j(x)}
    $$
    $w_i$ 大于$0$，相加为$1$。

######## 4.3 学习法

​		当训练数据很多的时候，使用学习法，即通过另一个学习器进行结合。

​		Stacking是学习发的典型代表。Stacking本身是一种著名的集成学习方法，也可以看做是一种结合策略。我们将个体学习器称为**初级学习器**，用于结合的学习器称为**次级学习器or元学习(meta-learner)**。

1）Stacking先从初始数据集训练出初级学习器。

2） 生成一个新的数据集训次级激学习器。为了防止训练次级训练器过拟合，这里一般采用交叉验证或者留一法这样的方式。即用训练初级学习器未使用的样板来产生刺激学习的训练样本。

​		这里其实就是先训练了 $T$ 个初级训练器，针对次级训练集中的 **$i$ 样本**，训练器  $t$ 会给出一个实数结果，$\mathbf z_i = (z_{i1}, z_{i2},...,z_{it})$，label还是 $y_i$， 所以 $(\mathbf z_i,y_i)$ 又构成了新的样本，也就是次级训练集。所谓的为了防止过拟合的操作是指这里的样板 **$i$ 样本**不能是初级训练器用过的训练集，可以是交叉验证的验证集。

#### 5. 多样性

###### 5.1 多样性增强

​		在集成学习中需要有效的生成多样性大的个体学习器。

-   **数据样本扰动**

    给定初始数据集，进行数据自己采样，训练出不同的个体学习器。通常是基于采样法。

-   **输入属性扰动**

    从初始属性集中抽取出若干个属性集，在基于每个属性子集训练基学习器。适用于包含大量冗余属性的数据。依据是从属性这个角度看，不同的属性“子空间”提供了观察数据的不同的视角，所以不同子空间训练出的数据集必然有所不同。同时，因为属性的减少，会大幅度节省时间开销。

-   **输出表示扰动**

    基本思路是对输出表示进行操纵以增强多样性。可以在训练的时候，直接就把label随机改变一下；也可以将输出表示进行转化，将分类输出转为回归输出；还可以将原任务分解为多个可同时求解的子任务。

-   **算法参数扰动**

​		通过随机设置不同的参数进行训练，单学习器使用交叉验证就已经是使用了不同的参数训练多个学习器，只不过最后会选择一个学习器进行使用。



