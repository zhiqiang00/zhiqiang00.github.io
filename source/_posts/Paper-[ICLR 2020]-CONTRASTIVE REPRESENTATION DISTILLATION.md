---
title: CONTRASTIVE REPRESENTATION DISTILLATION
tags: [对比学习, 知识蒸馏]
categories: [论文阅读]
katex: true
cover: https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/03/20/b36a188093daf2e64a217a84bf183201-nKO_1QyFh9o-2edcfd.jpg
top_img: https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/03/20/9d2244833e878e2169062087c9ab0874-wallhaven-g72p87-af7e51.jpg
---

## CONTRASTIVE REPRESENTATION DISTILLATION

## 摘要

我们常常希望将表征知识从一个神经网络转移到另一个神经网络。例如，将一个大的网络提炼成一个小的网络，将知识从一种感官形态转移到另一种感官形态，或将一组模型集成到单个估计器中。知识蒸馏，这些问题的标准方法，最小化教师和学生网络的概率输出之间的KL分歧。

动机：这一目标忽略了教师网络的重要结构知识。这激发了另一个目标，通过这个目标，我们训练学生在教师的数据表示中捕捉更多的信息。我们把这个目标定义为对比学习。

## 1 介绍

知识蒸馏(KD)将知识从一个深度学习模型(老师)转移到另一个深度学习模型(学生)。Hinton等人(2015)最初提出的目标是最小化教师和学生输出之间的KL分歧。当输出是一个分布时，这个公式具有直观意义，例如，类上的概率质量函数。

然而，我们往往希望转移有关表象的知识。例如，在“跨模态蒸馏”问题中，我们可能希望将图像处理网络的表示转移到声音(Aytar等人，2016)或深度(Gupta等人，2016)处理网络，这样图像的深度特征和相关的声音或深度特征高度相关。**在这种情况下，KL散度是没有定义的。**

具象知识是结构化的——维度表现出复杂的相互依赖关系。入的原始KD目标(**Hinton et al.， 2015**)将所有维度视为独立的，取决于输入。设y是老师的输出，y是学生的输出。那么原来的KD目标函数ψ，就具有完全分解的形式：![image-20220627164508210](https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/06/27/de5adc65a4567fa45977d11aceb46f7e-image-20220627164508210-ad596d.png)![image-20220627164537529](https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/06/27/183ef6396d0bb7a9427495cf75ba2303-image-20220627164537529-28c385.png)**这样的因子目标不足以传递结构知识，即输出维度i和j之间的依赖关系。**这类似于在图像生成中，L2目标由于输出维度之间的独立假设而产生模糊的结果。

为了克服这个问题，我们想要一个能够捕捉相关性和更高阶输出相关性的目标。为了实现这一点，在本文中，我们利用了一系列对比目标。近年来，这些目标函数已成功地用于密度估计和表示学习，特别是在自监督设置中。在这里，我们将它们用于从一个深度网络到另一个深度网络的知识提取任务。我们展示了在表示空间中工作的重要性。但是，请注意，在这些工作中使用的损失函数并没有显式地试图捕捉表征空间中的相关性或更高阶依赖关系。

我们的目标是最大化教师和学生表征之间的相互信息的下界。我们发现，这导致了更好的性能在几个知识转移任务。我们推测这是因为对比目标更好地传递了教师表征中的所有信息，而不是仅仅传递了关于条件独立输出类概率的知识。有些令人惊讶的是，对比目标甚至改进了最初提出的提取类概率知识的任务的结果，例如，将一个大型CIFAR100网络压缩成一个性能几乎相同的较小的网络。

我们的论文在两个基本独立发展的文献之间建立了联系:知识蒸馏和表征学习。这种联系使我们能够利用表征学习的强大方法来显著提高知识蒸馏方面的SOTA。我们的贡献是:

1. 一个基于对比用于在深度网络之间进行知识转移的目标函数。

2. 应用于模型压缩，跨模态传输和集成蒸馏。

   3.对12种最近的蒸馏方法进行基准测试;CRD优于所有其他方法，例如，与原始KD (Hinton等人，2015)†相比，平均相对改进57%，令人惊讶的是，后者的性能排名第二。

## 2 相关工作

Buciluˇa等人(2006)和Hinton等人(2015)的开创性工作在不失去太多泛化能力的情况下，引入了在大型、笨重的模型之间进行知识蒸馏的思想，将其转化为更小、更快的模型。一般的动机是，在训练时，计算的可用性允许模型的“slop”大小，并可能更快的学习。但是在推断时的计算和内存限制要求使用更小的模型(也就是说，动机是训练的时候可以使用更大的模型，并且可能更快的学习。利用知识蒸馏得到更小的模型后，满足在计算和内存现在的情况下的利用小模型进行推断)。Buciluˇa等人(2006)通过匹配输出对数来实现这一点;Hinton等人(2015)引入了softmax输出中的温度概念，以更好地表示单个样本输出中的较小概率。这些较小的概率提供了关于教师模型的学习表示的有用信息。在高温(会增加熵)低温之间的一些权衡，往往会在学生和老师之间提供最高的知识传递。

注意力转移(Zagoruyko & Komodakis, 2016a)侧重于网络的特征地图，而不是输出对数。这里的想法是在教师和学生的特征图(称为“注意力”)中引出类似的反应模式。然而，这种方法只能组合具有相同空间分辨率的特征地图，这是一个重大的限制，因为它需要具有非常相似的架构的学生和教师网络。这种技术实现了最先进的蒸馏结果(通过学生网络的泛化测量)。

## 3 方法

对比学习的关键思想非常普遍:学习在某些度量空间中接近“正”对的表示法，并将“负”对之间的表示法分开。图1直观地解释了我们如何为我们考虑的三个任务构建对比学习:模型压缩、跨模态迁移和集成蒸馏。

![image-20220627174842240](https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/06/27/8976bb4916d1000ab1163ce2b9878da7-image-20220627174842240-87d8da.png)

### 3.1 Contrastive Loss

给定两个深度神经网络，教师fT，学生fs，设x为网络输入;我们在倒数第二层(在对数之前)分别表示为fT (x)和fS(x)。设xi表示一个训练样本，xj表示另一个随机选择的样本。我们想把fS(xi)和fT(xi)的表达式推近，同时把fS(xi)和fT (xj)分开。

![image-20220627182711564](https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/06/27/88edfdb3d99eabc6a36f34c775c51c74-image-20220627182711564-d815d2.png)

直观地说，我们将考虑联合分布p(S, T)和边际分布p(S)p(T)的乘积，这样，通过最大化这些分布之间的KL散度，我们可以最大化学生和教师表示之间的互信息。为了设置一个适当的损失来实现这个目标，让我们定义一个带有潜在变量C的分布q，它决定元组(f T (xi)， f S(xj))是来自于联合(C = 1)还是边际(C = 0)的乘积:

![image-20220627182922176](https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/06/27/762e5ba829ec7c07c48522da7c3ba87b-image-20220627182922176-4a7148.png)

现在，假设在我们的数据中，对于每N个不全等对(来自边长的乘积;提供给T和S的独立随机抽取的输入)，则隐含C的先验为:

![image-20220627183537876](https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/06/27/b893112b54ad7970dc2a1515c7083f62-image-20220627183537876-705495.png)

通过简单操作和贝叶斯规则，C = 1类的后验为:

![image-20220627183853166](https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/06/27/12c3d2a3b606dcc6322390258d0a0970-image-20220627183853166-62f4c0.png)

接下来，我们观察到互信息的联系如下:

![image-20220627184333636](https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/06/27/77082ccd10936f0ad9324484ee70a171-image-20220627184333636-f6f68b.png)

我在哪里(T;S)是教师嵌入和学生嵌入分布之间的互信息。使Eq(T,S|C=1) log q(C =1 |T, S) w.r.t最大化，学生网络S的参数增加了互信息的下界。然而，我们不知道真实的分布q(C = 1|T, S);相反，我们通过拟合一个模型h: {T, S}→[0,1]来估计它，从数据分布q(C = 1|T, S)，其中T和S表示嵌入的域。我们最大化该模型下数据的对数似然(一个二元分类问题):

![image-20220627191750106](https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/06/27/f3f74ab775cda37f73c54a403cdf450e-image-20220627191750106-071ed0.png)

我们将h其命名为评论家，因为我们将学习优化评论家分数的表现形式，直接用10 11的公式替换了：

![image-20220627192139398](https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/06/27/6f6d3f67cdf68adb60b4558e9d2776b5-image-20220627192139398-76c169.png)

因此，我们看到最优评论家是一个期望下界互信息的估计量。我们希望学习一个学生，他的表示法和老师的表示法之间的互信息最大化，提出如下优化问题:

![image-20220627192318765](https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/06/27/a222b02fc04831f2ada84fb1ae33b2a2-image-20220627192318765-08370f.png)

这里一个明显的困难是，最优的评论家h∗取决于当前的学生。我们可以通过弱化(12)中的界来规避这个困难:

![image-20220627192409691](https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/06/27/96b824348120bdbb9c065d67b9933266-image-20220627192409691-d96898.png)

这表明我们可以在学习h的同时，共同优化f S。

我们可以选择用满足h的任何函数族来表示h: {T, S}→[0,1]。在实践中，我们使用以下方法:

![image-20220627192517474](https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/06/27/1c9e59ac3d952ff0b5fd4ffe717f0e63-image-20220627192517474-ed1585.png)

其中M是数据集的基数，τ是调节浓度水平的温度。

在实际应用中，由于S和T的维数可能不同，所以gS和gT将它们线性变换为相同的维数，并在内积之前通过L-2范数进一步归一化。Eq.(18)的形式受到NCE的启发(Gutmann & Hyvärinen, 2010;Wu et al.， 2018)。我们的公式类似于InfoNCE损失(Oord等人，2018)，因为我们最大化了互信息的下界。然而，我们使用了不同的目标和界限，在我们的实验中，我们发现这比InfoNCE更有效。

### 3.2 KNOWLEDGE DISTILLATION OBJECTIVE

Hinton等人(2015)提出了知识蒸馏损失。除了学生输出y和一热标签y之间的常规交叉熵损失之外，它通过最小化学生网络输出概率之间的交叉熵，要求学生网络输出尽可能与教师输出相似。完整的目标是:

![image-20220627192846837](https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/06/27/211a2bec792d26400aa54b3a0bfd287e-image-20220627192846837-199e00.png)

### 3.3 CROSS-MODAL TRANSFER LOSS 跨模态转移损失

### 3.4 ENSEMBLE DISTILLATION LOSS 集合蒸馏损失

在集合蒸馏如1(c)所示的情况下，我们有M > 1个教师网络，f Ti和一个学生网络f s。我们通过定义每个教师网络f Ti和学生网络f s的特征之间的多个成对的对比损失，采用对比框架。这些损失加起来得到最终的损失(最小化):

![image-20220627193353950](https://raw.githubusercontent.com/zhiqiang00/Picbed/main/blog-images/2022/06/27/412c46d3bd463e3dc564011094202420-image-20220627193353950-640886.png)

## 4 实验
