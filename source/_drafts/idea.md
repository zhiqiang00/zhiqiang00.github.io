对比学习本质上应该是要找到两个神似形不似的视图，然后进行最大化两个视图的互信息。

#### 1.有一个论文提出的是去干扰编码器，但是使用高斯分布添加的噪声，我认为这是不合理的。

我认为高阶信息是重要的，应该作为指导信息，其实应该把它当做最后的DGI里面的那个s，这样既可以引入高阶依赖信息，同时可以利用对比学习框架，充分发掘自身的语义信息。 这也算是一种增强的方式。

#### 2. 现在的对比学习里面的增强有一些方法是破坏节点属性或者进行子图采样，这是破坏了结构的

由于对比目标学习的表示往往不受数据增强方案引起的损坏的影响，我觉得应该找一种方法，在不破坏图结构和语义信息的情况下进行增强。例如考虑图的高阶信息，或者用PPR等进行采样。

或者  利用翠英师姐他们的思想，我首先删掉一些节点属性，然后预测，然后将带有原始属性和预测的属性的节点的表征进行对比。(这方方案的动机之一是有一些节点的属性是没有的，另外的动机就是增强数据)   $L = L_c + L_{com}$ 联合训练。



驱蚊子



**2022年https://ijcai-22.org/main-track-accepted-papers/**

总数 681

国内     10



天大       1 2 3   4 5  *Tianjin University,*





2021年https://ijcai-21.org/program-main-track/

总数 586

国内 10    20(245) 30(348) 40(498 )  50(648 )  60(788 )  70(853 )  80(975 )  90(1131 )  100(1319 )   110(1482 )  130(1796 )    150(2285 )

170(2741 ) 190(3537 )  210(4150 ) 230(5262 )  244

天大 1 2  3 4           *Tianjin University,*



2020年 https://dblp.org/db/conf/ijcai/ijcai2020.html

总数 592

国内     20 (537-543) 30(652-658)  40(745-752)   50( 825-831)  60 (920-926)  70(984)   80(1054-)  90(1177)   100(1359)  110( 1467)

120(1703)    130（2037） 140（2326）  150(2498)  170(2957)   180(3065)  200(3230)  210(3393)  230(3637)  250( 3794)  260(3941)

277

天大 12



2019年https://dblp.org/db/conf/ijcai/ijcai2019.html

总数 784 

国内  10（680） 20（774） 40 （912）    60（1312） 80（2137） 100（2463） 130（ 3059） 150（3446-   170（3849） 200(4171)

220 (4461)  304

天大 1 2 3 4 5 6 7 8 9



21年 总数586   国内244  天大4

20年 总数592  国内277  天大12

19年 总数784  国内304  天大9







4

















